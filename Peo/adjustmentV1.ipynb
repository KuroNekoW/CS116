{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea1ce598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 83 parquet files in the folder.\n",
      "\n",
      "Summary:\n",
      "  sales_pers.item: 1 files\n",
      "  sales_pers.purchase: 72 files\n",
      "  sales_pers.user: 10 files\n",
      "\n",
      "Reading 1 file(s) for group SALES ITEM...\n",
      "SALES ITEM loaded: 27,332 rows × 34 columns\n",
      "Reading 72 file(s) for group SALES PURCHASE...\n",
      "SALES PURCHASE loaded: 35,729,825 rows × 16 columns\n",
      "Reading 10 file(s) for group SALES USER...\n",
      "SALES USER loaded: 4,573,964 rows × 18 columns\n",
      "\n",
      "Dataset summary:\n",
      "sales_item_df: (27332, 34)\n",
      "sales_purchase_df: (35729825, 16)\n",
      "sales_user_df: (4573964, 18)\n",
      "sales_item_df new shape: (27332, 22)\n",
      "sales_purchase_df new shape: (35729825, 14)\n",
      "sales_user_df new shape: (4573964, 14)\n",
      "\n",
      "Outlier capping complete for all datasets.\n",
      "\n",
      "High correlation columns removed for all datasets.\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# === Path to dataset folder ===\n",
    "base_path = Path(r\"D:\\recommendation dataset\")\n",
    "\n",
    "# === Find all parquet files ===\n",
    "all_parquet_files = list(base_path.glob(\"*.parquet\"))\n",
    "print(f\"Found {len(all_parquet_files)} parquet files in the folder.\")\n",
    "\n",
    "# === Classify files by type ===\n",
    "sales_item_files = []\n",
    "sales_purchase_files = []\n",
    "sales_user_files = []\n",
    "\n",
    "for file_path in all_parquet_files:\n",
    "    name = file_path.name.lower()\n",
    "    if \"sales_pers.item\" in name:\n",
    "        sales_item_files.append(file_path)\n",
    "    elif \"sales_pers.purchase\" in name or \"sales_pers.purchase_history_daily\" in name:\n",
    "        sales_purchase_files.append(file_path)\n",
    "    elif \"sales_pers.user\" in name:\n",
    "        sales_user_files.append(file_path)\n",
    "\n",
    "print(f\"\"\"\n",
    "Summary:\n",
    "  sales_pers.item: {len(sales_item_files)} files\n",
    "  sales_pers.purchase: {len(sales_purchase_files)} files\n",
    "  sales_pers.user: {len(sales_user_files)} files\n",
    "\"\"\")\n",
    "\n",
    "# === Function to read parquet files ===\n",
    "def read_parquet_group(file_list, group_name):\n",
    "    if not file_list:\n",
    "        print(f\"No files for group {group_name}\")\n",
    "        return None\n",
    "    print(f\"Reading {len(file_list)} file(s) for group {group_name}...\")\n",
    "    df = pl.read_parquet(file_list)\n",
    "    print(f\"{group_name} loaded: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "    return df\n",
    "\n",
    "# === Read dataframes ===\n",
    "sales_item_df = read_parquet_group(sales_item_files, \"SALES ITEM\")\n",
    "sales_purchase_df = read_parquet_group(sales_purchase_files, \"SALES PURCHASE\")\n",
    "sales_user_df = read_parquet_group(sales_user_files, \"SALES USER\")\n",
    "\n",
    "# === Print dataset shapes ===\n",
    "print(\"\\nDataset summary:\")\n",
    "if sales_item_df is not None:\n",
    "    print(f\"sales_item_df: {sales_item_df.shape}\")\n",
    "if sales_purchase_df is not None:\n",
    "    print(f\"sales_purchase_df: {sales_purchase_df.shape}\")\n",
    "if sales_user_df is not None:\n",
    "    print(f\"sales_user_df: {sales_user_df.shape}\")\n",
    "\n",
    "# === Columns to drop ===\n",
    "cols_to_drop = list(set([\n",
    "    # SALES ITEM\n",
    "    \"is_deleted\", \"last_sync_date\", \"sync_error_message\", \"image_url\",\n",
    "    \"description_new\", \"weight\", \"sync_status_id\", \"category_l1_id\",\n",
    "    \"category_l2_id\", \"category_l3_id\", \"category_id\", \"manufacturer\",\n",
    "    # SALES PURCHASE\n",
    "    \"is_deleted\", \"event_type\",\n",
    "    # SALES USER\n",
    "    \"is_deleted\", \"sync_status_id\", \"last_sync_date\", \"sync_error_message\"\n",
    "]))\n",
    "\n",
    "# === Drop columns directly ===\n",
    "if sales_item_df is not None:\n",
    "    existing_cols = [c for c in cols_to_drop if c in sales_item_df.columns]\n",
    "    if existing_cols:\n",
    "        sales_item_df = sales_item_df.drop(existing_cols)\n",
    "    print(f\"sales_item_df new shape: {sales_item_df.shape}\")\n",
    "\n",
    "if sales_purchase_df is not None:\n",
    "    existing_cols = [c for c in cols_to_drop if c in sales_purchase_df.columns]\n",
    "    if existing_cols:\n",
    "        sales_purchase_df = sales_purchase_df.drop(existing_cols)\n",
    "    print(f\"sales_purchase_df new shape: {sales_purchase_df.shape}\")\n",
    "\n",
    "if sales_user_df is not None:\n",
    "    existing_cols = [c for c in cols_to_drop if c in sales_user_df.columns]\n",
    "    if existing_cols:\n",
    "        sales_user_df = sales_user_df.drop(existing_cols)\n",
    "    print(f\"sales_user_df new shape: {sales_user_df.shape}\")\n",
    "\n",
    "# === Numeric columns for outlier capping ===\n",
    "target_columns = {\n",
    "    \"SALES ITEM\": ['price', 'creation_timestamp', 'gp'],\n",
    "    \"SALES PURCHASE\": ['price'],\n",
    "    \"SALES USER\": ['timestamp', 'install_date']\n",
    "}\n",
    "\n",
    "# === Function to cap outliers using IQR ===\n",
    "def cap_outliers(df, cols):\n",
    "    df_result = df.clone()\n",
    "    for col in cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        try:\n",
    "            _ = df[col].to_numpy().astype(float)\n",
    "        except:\n",
    "            continue\n",
    "        Q1, Q3 = df[col].quantile(0.25), df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_iqr, upper_iqr = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
    "        df_result = df_result.with_columns(\n",
    "            pl.when(pl.col(col) < lower_iqr).then(lower_iqr)\n",
    "            .when(pl.col(col) > upper_iqr).then(upper_iqr)\n",
    "            .otherwise(pl.col(col)).alias(col)\n",
    "        )\n",
    "    return df_result\n",
    "\n",
    "# === Apply outlier capping ===\n",
    "if sales_item_df is not None:\n",
    "    sales_item_df = cap_outliers(sales_item_df, target_columns[\"SALES ITEM\"])\n",
    "if sales_purchase_df is not None:\n",
    "    sales_purchase_df = cap_outliers(sales_purchase_df, target_columns[\"SALES PURCHASE\"])\n",
    "if sales_user_df is not None:\n",
    "    sales_user_df = cap_outliers(sales_user_df, target_columns[\"SALES USER\"])\n",
    "\n",
    "print(\"\\nOutlier capping complete for all datasets.\")\n",
    "\n",
    "# === Remove highly correlated columns ===\n",
    "max_sample = 100_000\n",
    "\n",
    "def remove_high_corr(df, threshold=0.85):\n",
    "    num_cols = [col for col, dtype in zip(df.columns, df.dtypes) if dtype in pl.NUMERIC_DTYPES]\n",
    "    if len(num_cols) < 2:\n",
    "        return df\n",
    "    df_sample = df.select(num_cols)\n",
    "    if df_sample.height > max_sample:\n",
    "        df_sample = df_sample.sample(n=max_sample, seed=42)\n",
    "    corr_matrix = df_sample.to_pandas().corr().abs()\n",
    "    mask = np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    high_corr_pairs = (\n",
    "        corr_matrix.where(mask)\n",
    "        .stack()\n",
    "        .reset_index()\n",
    "        .rename(columns={0: \"correlation\", \"level_0\": \"feature_1\", \"level_1\": \"feature_2\"})\n",
    "        .query(\"correlation > @threshold\")\n",
    "    )\n",
    "    to_drop = set(high_corr_pairs[\"feature_2\"].tolist())\n",
    "    if to_drop:\n",
    "        df = df.drop(list(to_drop))\n",
    "    return df\n",
    "\n",
    "if sales_item_df is not None:\n",
    "    sales_item_df = remove_high_corr(sales_item_df)\n",
    "\n",
    "if sales_purchase_df is not None:\n",
    "    sales_purchase_df = remove_high_corr(sales_purchase_df)\n",
    "\n",
    "if sales_user_df is not None:\n",
    "    sales_user_df = remove_high_corr(sales_user_df)\n",
    "\n",
    "print(\"\\nHigh correlation columns removed for all datasets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23118e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['p_id', 'item_id', 'price', 'category_l1', 'category_l2', 'category_l3', 'category', 'description', 'brand', 'created_date', 'updated_date', 'gender_target', 'age_group', 'item_type', 'gp', 'color', 'size', 'origin', 'volume', 'material', 'sale_status']\n",
      "['timestamp', 'user_id', 'item_id', 'event_value', 'price', 'customer_id', 'created_date', 'updated_date', 'channel', 'payment', 'location', 'discount']\n",
      "['customer_id', 'gender', 'location', 'province', 'membership', 'created_date', 'updated_date', 'region', 'location_name', 'install_app', 'install_date', 'district', 'user_id']\n"
     ]
    }
   ],
   "source": [
    "#Cho em Hoe cute\n",
    "\n",
    "print(sales_item_df.columns)\n",
    "print(sales_purchase_df.columns)\n",
    "print(sales_user_df.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
